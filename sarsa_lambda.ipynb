{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855b363c",
   "metadata": {},
   "source": [
    "# Sarsa(λ) – Forward View (λ‑return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef363e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, matplotlib.pyplot as plt, pickle, pathlib\n",
    "from env import BlackjackEnv\n",
    "from tqdm import trange\n",
    "from collections import defaultdict\n",
    "\n",
    "env = BlackjackEnv(seed=1)\n",
    "\n",
    "# Load MC ground truth\n",
    "with open('mc_Q_star.pkl', 'rb') as f:\n",
    "    Q_star = pickle.load(f)\n",
    "\n",
    "def mse(Q_hat):\n",
    "    err = 0.0\n",
    "    for s_a,q_star in Q_star.items():\n",
    "        err += (Q_hat.get(s_a, np.zeros(2))[s_a[1]] - q_star)**2 if isinstance(s_a, tuple) else 0\n",
    "    return err / len(Q_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6186736",
   "metadata": {},
   "source": [
    "### Helper: λ‑return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lambda_return(rewards, t, lam):\n",
    "    G = 0.0\n",
    "    lam_pow = 1.0\n",
    "    for n in range(1, len(rewards)-t):\n",
    "        G_n = rewards[t+1:t+n+1].sum()\n",
    "        G += lam_pow * (1-lam) * G_n\n",
    "        lam_pow *= lam\n",
    "    G += lam_pow * rewards[-1]\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b0f35",
   "metadata": {},
   "source": [
    "### Run experiments for λ ∈ {0,0.1,…,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lam_grid = np.linspace(0,1,11)\n",
    "mse_per_lam = []\n",
    "episodes = 1000\n",
    "for lam in lam_grid:\n",
    "    Q_hat = defaultdict(lambda: np.zeros(2))\n",
    "    alpha = 0.01\n",
    "    mses = []\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        episode_states, episode_actions, episode_rewards = [], [], [0]\n",
    "        done=False\n",
    "        # generate episode\n",
    "        while not done:\n",
    "            # ε-greedy with constant ε=0.1\n",
    "            if np.random.rand() < 0.1:\n",
    "                a = np.random.randint(2)\n",
    "            else:\n",
    "                q_vals = Q_hat[(s[0], s[1])]\n",
    "                a = np.random.choice(np.flatnonzero(q_vals == q_vals.max()))\n",
    "            episode_states.append(s)\n",
    "            episode_actions.append(a)\n",
    "            s_next, r, done = env.step(a)\n",
    "            episode_rewards.append(r)\n",
    "            s = s_next if not done else (None, None)\n",
    "        # forward view updates\n",
    "        T = len(episode_states)\n",
    "        for t in range(T):\n",
    "            G_lam = lambda_return(np.array(episode_rewards), t, lam)\n",
    "            key = (episode_states[t][0], episode_states[t][1]), episode_actions[t]\n",
    "            Q_hat[key[0]][key[1]] += alpha * (G_lam - Q_hat[key[0]][key[1]])\n",
    "        # log mse\n",
    "        mses.append(mse(Q_hat))\n",
    "    mse_per_lam.append(mses[-1])\n",
    "    print(f\"λ={lam:.1f}  final MSE={mses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672eff81",
   "metadata": {},
   "source": [
    "### Plot MSE vs λ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb4a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(lam_grid, mse_per_lam, marker='o')\n",
    "plt.xlabel('λ')\n",
    "plt.ylabel('MSE to Q★')\n",
    "plt.title('Mean‑squared error after 1000 episodes')\n",
    "pathlib.Path('plots').mkdir(exist_ok=True)\n",
    "plt.savefig('plots/mse_vs_lambda.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
